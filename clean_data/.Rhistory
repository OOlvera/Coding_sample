str_remove(global_health_expenditure$diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
str_remove(global_health_expenditure$diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
str_remove(diabetes,"-")
global_health_expenditure <- read.csv(file.path(path_clean,"df_final.csv"))
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
str_remove(diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
diabetes=str_remove(diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
global_health_expenditure$diabetes=str_remove(global_health_expenditure$diabetes,"-")
library(tidyverse)
library(tidyverse)
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
global_health_expenditure$diabetes=str_remove(global_health_expenditure$diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
diabetes=str_remove(global_health_expenditure$diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019)%>%
diabetes=str_remove(diabetes,"-")
global_health_expenditure %>%
filter(date==2000 |date== 2011 |date== 2019
global_health_expenditure %>%
# Fall 2022
# Final Project - KARLA, ASTRID & OLIVIA
# Get Data sources
library(tidyverse)
library(ggplot2)
library(readxl)
library(readr)
library(dplyr)
library(hrbrthemes)
library(ggsci)
library(ggpubr)
library(rstatix)
library(tidytext)
library(stringr)
library(knitr)
library(DT)
library(tm)
library(tidyr)
library(wbstats)
library(syuzhet)
library(rvest)
library (xml2)
library(rvest)
library(tidytext)
library(textdata)
library(lubridate)
# install.packages("wbstats")
# Set paths
path_raw <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/raw_data/"
path_clean <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/"
## Data source 1
## From the World Bank API
## Data of life expectancy, out-of-pocket expenditure, GDP and health expenditure (% of GDP) by country
start_date = 2000
end_date = 2019
latam <- c("ABW","ARG","ATG","BHS","BLZ","BOL","BRA","BRB","CHL","COL","CRI","CUB","CUW","CYM",
"DMA","DOM","ECU","GRD","GTM","GUY","HND","HTI","JAM","KNA","LCA","MAF","MEX","NIC",
"PAN","PER","PRI","PRY","SLV","SUR","SXM","TCA","TTO","URY","VCT","VEN","VGB","VIR")
indicators <- c("SH.XPD.OOPC.CH.ZS", "NY.GDP.PCAP.CD", "SP.DYN.LE00.IN", "SH.XPD.CHEX.PP.CD")
get_state <- function(ind) {
s <- wb_data(ind, start_date = start_date, end_date = end_date) %>%
select(iso3c, country, date, ind)
}
data <- list()
i <- 1
for (ind in indicators) {
data[[i]] <- get_state(ind)
i = i + 1
}
df <- reduce(data, left_join, by = c("iso3c", "date", "country")) %>%
filter(iso3c  %in% latam) %>%
rename(`out_of_pocket` = `SH.XPD.OOPC.CH.ZS`,
`gdp_percap` = `NY.GDP.PCAP.CD`,
`life_exp` = `SP.DYN.LE00.IN`,
`health_exp_percap` = `SH.XPD.CHEX.PP.CD`)
## Data source 2
## CSV of diabetes prevalence
## Source: https://diabetesatlas.org/data/en/indicators/2/
df_diabetes = read.csv(file.path(path_raw,"IDF (age-adjusted-comparative-prevalence-of-diabetes---).csv")) %>%
select(Country.Territory, X2000, X2011, X2021) %>%
rename(`X2019` = `X2021`,
`country` = `Country.Territory`) %>%
pivot_longer(cols = -c(country), names_to = "date", values_to = "diabetes")
df_diabetes$date <- substring(df_diabetes$date, 2)
df_diabetes <- df_diabetes %>%
mutate_at('date', as.numeric)
df_final <- df %>%
left_join(df_diabetes, by = c("country", "date"))
write.csv(df_final, file.path(path_clean, "df_final.csv"), row.names = FALSE)
headers = c("expenditure_type", "country", "date", "expenditure")
health_exp_mex = read.csv(file.path(path_raw, "Interactivos20221130232334.csv"), header = F)
colnames(health_exp_mex) = headers
health_exp_mex <- health_exp_mex %>%
filter(country!="") %>%
separate(expenditure_type, into = paste0('type', 1:3), sep = '[.]') %>%
separate(type3, into = paste0('exp_type', 1:2), sep = '[(]') %>%
select(date, exp_type1, expenditure)
View(health_exp_mex)
health_exp_mex <- health_exp_mex %>%
filter(country!="") %>%
separate(expenditure_type, into = paste0('type', 1:3), sep = '[.]') %>%
separate(type3, into = paste0('exp_type', 1:2), sep = '[(]') %>%
select(date, exp_type1, expenditure) %>%
mutate(exp_type1 = str_replace(exp_type1, "Consultas médicas ", "Doctor visits"),
exp_type1 = str_replace(exp_type1, "Bienes de apoyo ", "Suppost items"),
exp_type1 = str_replace(exp_type1, "Servicios de apoyo ", "Support care"),
exp_type1 = str_replace(exp_type1, "Servicios hospitalarios ", "Inpatient care"),
exp_type1 = str_replace(exp_type1, "Medicamentos y otros bienes ", "Drugs"))
## Source: https://www.inegi.org.mx/app/tabulados/pxwebclient/default.html?pxq=BISE_BISE_Ac0CuayZ_220713150914_7e479a70-05a1-4689-95be-2441cc671ad7
headers = c("expenditure_type", "country", "date", "expenditure")
health_exp_mex = read.csv(file.path(path_raw, "Interactivos20221130232334.csv"), header = F)
colnames(health_exp_mex) = headers
health_exp_mex <- health_exp_mex %>%
filter(country!="") %>%
separate(expenditure_type, into = paste0('type', 1:3), sep = '[.]') %>%
separate(type3, into = paste0('exp_type', 1:2), sep = '[(]') %>%
select(date, exp_type1, expenditure) %>%
mutate(exp_type1 = str_replace(exp_type1, "Consultas médicas ", "Doctor visits"),
exp_type1 = str_replace(exp_type1, "Bienes de apoyo ", "Suppost items"),
exp_type1 = str_replace(exp_type1, "Servicios de apoyo ", "Support care"),
exp_type1 = str_replace(exp_type1, "Servicios hospitalarios ", "Inpatient care"),
exp_type1 = str_replace(exp_type1, "Medicamentos y otros bienes ", "Drugs"))
View(health_exp_mex)
# Fall 2022
# Final Project - KARLA, ASTRID & OLIVIA
# Get Data sources
library(tidyverse)
library(ggplot2)
library(readxl)
library(readr)
library(dplyr)
library(hrbrthemes)
library(ggsci)
library(ggpubr)
library(rstatix)
library(tidytext)
library(stringr)
library(knitr)
library(DT)
library(tm)
library(tidyr)
library(wbstats)
library(syuzhet)
library(rvest)
library (xml2)
library(rvest)
library(tidytext)
library(textdata)
library(lubridate)
# install.packages("wbstats")
# Set paths
path_raw <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/raw_data/"
path_clean <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/"
## Data source 1
## From the World Bank API
## Data of life expectancy, out-of-pocket expenditure, GDP and health expenditure (% of GDP) by country
start_date = 2000
end_date = 2019
latam <- c("ABW","ARG","ATG","BHS","BLZ","BOL","BRA","BRB","CHL","COL","CRI","CUB","CUW","CYM",
"DMA","DOM","ECU","GRD","GTM","GUY","HND","HTI","JAM","KNA","LCA","MAF","MEX","NIC",
"PAN","PER","PRI","PRY","SLV","SUR","SXM","TCA","TTO","URY","VCT","VEN","VGB","VIR")
indicators <- c("SH.XPD.OOPC.CH.ZS", "NY.GDP.PCAP.CD", "SP.DYN.LE00.IN", "SH.XPD.CHEX.PP.CD")
get_state <- function(ind) {
s <- wb_data(ind, start_date = start_date, end_date = end_date) %>%
select(iso3c, country, date, ind)
}
data <- list()
i <- 1
for (ind in indicators) {
data[[i]] <- get_state(ind)
i = i + 1
}
df <- reduce(data, left_join, by = c("iso3c", "date", "country")) %>%
filter(iso3c  %in% latam) %>%
rename(`out_of_pocket` = `SH.XPD.OOPC.CH.ZS`,
`gdp_percap` = `NY.GDP.PCAP.CD`,
`life_exp` = `SP.DYN.LE00.IN`,
`health_exp_percap` = `SH.XPD.CHEX.PP.CD`)
## Data source 2
## CSV of diabetes prevalence
## Source: https://diabetesatlas.org/data/en/indicators/2/
df_diabetes = read.csv(file.path(path_raw,"IDF (age-adjusted-comparative-prevalence-of-diabetes---).csv")) %>%
select(Country.Territory, X2000, X2011, X2021) %>%
rename(`X2019` = `X2021`,
`country` = `Country.Territory`) %>%
pivot_longer(cols = -c(country), names_to = "date", values_to = "diabetes")
df_diabetes$date <- substring(df_diabetes$date, 2)
df_diabetes <- df_diabetes %>%
mutate_at('date', as.numeric)
df_final <- df %>%
left_join(df_diabetes, by = c("country", "date"))
write.csv(df_final, file.path(path_clean, "df_final.csv"), row.names = FALSE)
## Data source 3
## CSV from the Mexican Institute of Statistics (INEGI)
## Disaggreation out-of-pocket expenditure, just Mexico
## Source: https://www.inegi.org.mx/app/tabulados/pxwebclient/default.html?pxq=BISE_BISE_Ac0CuayZ_220713150914_7e479a70-05a1-4689-95be-2441cc671ad7
headers = c("expenditure_type", "country", "date", "expenditure")
health_exp_mex = read.csv(file.path(path_raw, "Interactivos20221130232334.csv"), header = F)
colnames(health_exp_mex) = headers
health_exp_mex <- health_exp_mex %>%
filter(country!="") %>%
separate(expenditure_type, into = paste0('type', 1:3), sep = '[.]') %>%
separate(type3, into = paste0('exp_type', 1:2), sep = '[(]') %>%
select(date, exp_type1, expenditure) %>%
mutate(exp_type1 = str_replace(exp_type1, "Consultas médicas ", "Doctor visits"),
exp_type1 = str_replace(exp_type1, "Bienes de apoyo ", "Suppost items"),
exp_type1 = str_replace(exp_type1, "Servicios de apoyo ", "Support care"),
exp_type1 = str_replace(exp_type1, "Servicios hospitalarios ", "Inpatient care"),
exp_type1 = str_replace(exp_type1, "Medicamentos y otros bienes ", "Drugs"))
write.csv(health_exp_mex, file.path(path_clean, "health_exp_mex.csv"), row.names = FALSE)
## Data source 4
## ZIP directly from the url
## Name and location of all private primary care centers in Mexico
## Note that the file is too heavy (20MB) and might take time to load
## Source: https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data
temp <- tempfile()
filename = "https://www.inegi.org.mx/contenidos/masiva/denue/2022_05/denue_00_62_0522_csv.zip"
csvfile = "conjunto_de_datos/denue_inegi_62_.csv"
download.file(filename,temp, mode = "wb")
unzip(temp, csvfile)
nlp_data <- read.table(csvfile, sep = ",", header = T)
unlink(temp)
nlp_data <- nlp_data %>%
filter(nombre_act == 'Consultorios de medicina general del sector privado') %>%
select(nom_estab, nombre_act, entidad, latitud, longitud)
write.csv(nlp_data, file.path(path_clean, "nlp_data.csv"), row.names = FALSE)
## Data source 5
## Get news from Google Search
## Extract news from Google - Search "Mexico out-of-pocket")
## Sources: https://www.listendata.com/2020/12/web-scrape-google-news-with-r.html
## https://allanvc.github.io/post/2018-08-21-google_news_scraping/
news_outofpocket <- read_html("https://news.google.com/search?q=mexico%20out%20of%20pocket%20health&hl=en-US&gl=US&ceid=US%3Aen")
# Get headlines
headlines <- data.frame(title = news_outofpocket %>%
html_nodes('.DY5T1d') %>%
html_text())
# Get time of each row
prod <- html_nodes(news_outofpocket, ".SVJrMe")
time <- lapply(prod, function(x) {
norm <- tryCatch(html_node(x, "time") %>% html_text(),
error=function(err) {NA})
})
# Create news data frame
time <- data.frame(time = do.call(rbind, time), stringsAsFactors = F)
news_nlp <- cbind(headlines, time)
write.csv(news_nlp, file.path(path_clean, "news_nlp.csv"), row.names = FALSE)
mexico_shape <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"))
# Fall 2022
# Final Project - KARLA, ASTRID & OLIVIA
# Text Processing
library(tidyverse)
library(ggplot2)
library(tidytext)
library(stringr)
library(wordcloud)
library(sf)
library(rnaturalearth)
library(mxmaps)
library(syuzhet)
#devtools::install_github("diegovalle/mxmaps")
setwd("C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/")
path_raw <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/raw_data/"
path_clean <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/"
path_plot <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/plots/"
mexico_shape <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"))
sf <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"), options = "ENCODING=WINDOWS-1252")
View(mexico_shape)
View(sf)
sf <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"), options = "UTF-8")
View(sf)
?ST_READ
?st_read
rm(list = ls())
library(tidyverse)
library(ggplot2)
library(tidytext)
library(stringr)
library(wordcloud)
library(sf)
library(rnaturalearth)
library(mxmaps)
library(syuzhet)
#devtools::install_github("diegovalle/mxmaps")
setwd("C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/")
path_raw <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/raw_data/"
path_clean <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/"
path_plot <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/plots/"
## From Data source 4
mexico_shape <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"))
sf <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"), options = "ENCODING=UTF-8")
View(mexico_shape)
View(sf)
sf <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"))
mexico_shape <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"), options = "ENCODING=UTF-8")
primary_care_final<- left_join(mexico_shape, primary_care, by='ADM1_ES')
map_format <- scale_fill_gradientn(colours = hcl.colors(3, "GnBu", rev = TRUE),n.breaks = 4)
ggplot() +
geom_sf(data = primary_care_final, aes(fill = total_pc)) +
map_format +
labs(title="Total private primary care centers",
fill = "Total") +
theme_void()
primary_care <- nlp_data %>%
group_by(entidad) %>%
summarise(total_pc = n())
primary_care$ADM1_ES <- str_to_title(primary_care$entidad)
primary_care$ADM1_ES <-str_trim(primary_care$ADM1_ES, side = c("right"))
primary_care <- primary_care %>%
mutate(ADM1_ES = str_replace(ADM1_ES, "Veracruz De Ignacio De La Llave", "Veracruz de Ignacio de la Llave"),
ADM1_ES = str_replace(ADM1_ES, "Michoacán De Ocampo", "Michoacán de Ocampo"),
ADM1_ES = str_replace(ADM1_ES, "Coahuila De Zaragoza", "Coahuila de Zaragoza"),
ADM1_ES = str_replace(ADM1_ES, "Ciudad De México", "Distrito Federal"),
ADM1_ES = str_replace(ADM1_ES, "Querétaro", "Querétaro de Arteaga"))
nlp_data <- read.csv("nlp_data.csv")
primary_care <- nlp_data %>%
group_by(entidad) %>%
summarise(total_pc = n())
primary_care$ADM1_ES <- str_to_title(primary_care$entidad)
primary_care$ADM1_ES <-str_trim(primary_care$ADM1_ES, side = c("right"))
primary_care <- primary_care %>%
mutate(ADM1_ES = str_replace(ADM1_ES, "Veracruz De Ignacio De La Llave", "Veracruz de Ignacio de la Llave"),
ADM1_ES = str_replace(ADM1_ES, "Michoacán De Ocampo", "Michoacán de Ocampo"),
ADM1_ES = str_replace(ADM1_ES, "Coahuila De Zaragoza", "Coahuila de Zaragoza"),
ADM1_ES = str_replace(ADM1_ES, "Ciudad De México", "Distrito Federal"),
ADM1_ES = str_replace(ADM1_ES, "Querétaro", "Querétaro de Arteaga"))
sf <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"))
mexico_shape <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"), options = "ENCODING=UTF-8")
primary_care_final<- left_join(mexico_shape, primary_care, by='ADM1_ES')
map_format <- scale_fill_gradientn(colours = hcl.colors(3, "GnBu", rev = TRUE),n.breaks = 4)
ggplot() +
geom_sf(data = primary_care_final, aes(fill = total_pc)) +
map_format +
labs(title="Total private primary care centers",
fill = "Total") +
theme_void()
pop_data <- df_mxstate_2020 %>%
mutate(state_name_official = str_replace(state_name_official, "Querétaro", "Querétaro de Arteaga"),
state_name_official = str_replace(state_name_official, "Ciudad de México", "Distrito Federal"))
pop_data$ADM1_ES <- pop_data$state_name_official
primary_care_final<- primary_care_final %>%
left_join(pop_data, by='ADM1_ES') %>%
mutate(total_pc_r = (total_pc/pop)* 100)
ggplot() +
geom_sf(data = primary_care_final, aes(fill = total_pc_r)) +
map_format +
labs(title="Private primary care centers (relative to population)",
fill = "Percentage") +
theme_void()
# Fall 2022
# Final Project - KARLA, ASTRID & OLIVIA
# Text Processing
library(tidyverse)
library(ggplot2)
library(tidytext)
library(stringr)
library(wordcloud)
library(sf)
library(rnaturalearth)
library(mxmaps)
library(syuzhet)
#devtools::install_github("diegovalle/mxmaps")
setwd("C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/")
path_raw <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/raw_data/"
path_clean <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/clean_data/"
path_plot <- "C:/Users/olivi/Box/FALL 2022/Data and Programming/final-project-karla-olivia-astrid/plots/"
## From Data source 4
## Objective: Text processing of the primary care centers in Mexico
## Source: Mexican National Institute of Statistics
## Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/
nlp_data <- read.csv("nlp_data.csv")
## Delete stop words and additional common words not useful for the analysis. The words are in Spanish
custom_stop_words <- bind_rows(stop_words, data_frame(word = tm::stopwords("spanish"), lexicon = "custom"))
uni_sw <- data.frame(word = c("consultorio","medico","nombre", "medicina", "médico", "consultorios", "medica", "consultorios"))
nlp_words <- nlp_data %>%
unnest_tokens(word, nom_estab) %>%
anti_join(custom_stop_words) %>%
anti_join(uni_sw, by = "word") %>%
select(word) %>%
count(word, sort = TRUE) %>%
ungroup()
## Visualizations of most common names/words for private primary care centers
## 1. WordCloud
## Source: https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/
## The WordCloud shows that most of the primary care centers are located in pharmacies ("farmacia")
## This is important for healthcare policies
pal <- brewer.pal(8,"Dark2")
png(file.path(path_plot,"wordcloud_plot.png"))
nlp_words %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 40, colors=pal))
dev.off()
## 2. Maps
## Data wrangling based on the NLP data
primary_care <- nlp_data %>%
group_by(entidad) %>%
summarise(total_pc = n())
primary_care$ADM1_ES <- str_to_title(primary_care$entidad)
primary_care$ADM1_ES <-str_trim(primary_care$ADM1_ES, side = c("right"))
primary_care <- primary_care %>%
mutate(ADM1_ES = str_replace(ADM1_ES, "Veracruz De Ignacio De La Llave", "Veracruz de Ignacio de la Llave"),
ADM1_ES = str_replace(ADM1_ES, "Michoacán De Ocampo", "Michoacán de Ocampo"),
ADM1_ES = str_replace(ADM1_ES, "Coahuila De Zaragoza", "Coahuila de Zaragoza"),
ADM1_ES = str_replace(ADM1_ES, "Ciudad De México", "Distrito Federal"),
ADM1_ES = str_replace(ADM1_ES, "Querétaro", "Querétaro de Arteaga"))
## Map 1: Total number of private primary care centers by state (absolute numbers)
mexico_shape <- st_read(file.path(path_raw, "mexican_states_shapefile.shp"), options = "ENCODING=UTF-8")
primary_care_final<- left_join(mexico_shape, primary_care, by='ADM1_ES')
map_format <- scale_fill_gradientn(colours = hcl.colors(3, "GnBu", rev = TRUE),n.breaks = 4)
png(file.path(path_plot,"map_1.png"))
ggplot() +
geom_sf(data = primary_care_final, aes(fill = total_pc)) +
map_format +
labs(title="Total private primary care centers",
fill = "Total") +
theme_void()
dev.off()
## Map 2: Total number of private primary care centers by state (controlled by population)
## population data from the library mxmaps
pop_data <- df_mxstate_2020 %>%
mutate(state_name_official = str_replace(state_name_official, "Querétaro", "Querétaro de Arteaga"),
state_name_official = str_replace(state_name_official, "Ciudad de México", "Distrito Federal"))
pop_data$ADM1_ES <- pop_data$state_name_official
primary_care_final<- primary_care_final %>%
left_join(pop_data, by='ADM1_ES') %>%
mutate(total_pc_r = (total_pc/pop)* 100)
png(file.path(path_plot,"map_2.png"))
ggplot() +
geom_sf(data = primary_care_final, aes(fill = total_pc_r)) +
map_format +
labs(title="Private primary care centers (relative to population)",
fill = "Percentage") +
theme_void()
dev.off()
## From Data source 5
## Objective: NRC Sentiment Analysis of Mexican news about out-of-pocket
news_nlp <- read.csv("news_nlp.csv")
# NRC as the chosen sentiment package
sentiment_nrc <- get_sentiments("nrc") %>%
rename(nrc = sentiment)
words_df <- tibble(text = news_text) %>%
mutate_all(as.character) %>%
unnest_tokens(word_tokens, text, token="words") %>%
anti_join(stop_words, by = c("word_tokens" = "word")) %>%
left_join(sentiment_nrc, by = c("word_tokens" = "word")) %>%
filter(nrc != "positive" & nrc != "negative")
# Plotting the NRC results
png(file.path(path_plot,"NRC_plot.png"))
ggplot(data = filter(words_df, !is.na(nrc))) +
geom_histogram(aes(nrc), stat = "count", fill="Navy") +
scale_x_discrete(guide = guide_axis(angle = 45)) +
labs(title = "Overall sentiment analysis: Mexican news about out-of-pocket",
subtitle = "NRC method") +
theme_minimal()
dev.off()
news_nlp <- read.csv("news_nlp.csv")
# NRC as the chosen sentiment package
sentiment_nrc <- get_sentiments("nrc") %>%
rename(nrc = sentiment)
words_df <- tibble(text = news_nlp) %>%
mutate_all(as.character) %>%
unnest_tokens(word_tokens, text, token="words") %>%
anti_join(stop_words, by = c("word_tokens" = "word")) %>%
left_join(sentiment_nrc, by = c("word_tokens" = "word")) %>%
filter(nrc != "positive" & nrc != "negative")
# Plotting the NRC results
png(file.path(path_plot,"NRC_plot.png"))
ggplot(data = filter(words_df, !is.na(nrc))) +
geom_histogram(aes(nrc), stat = "count", fill="Navy") +
scale_x_discrete(guide = guide_axis(angle = 45)) +
labs(title = "Overall sentiment analysis: Mexican news about out-of-pocket",
subtitle = "NRC method") +
theme_minimal()
dev.off()
words_df <- tibble(text = news_nlp) %>%
mutate_all(as.character) %>%
unnest_tokens(word_tokens, text, token="words") %>%
anti_join(stop_words, by = c("word_tokens" = "word")) %>%
left_join(sentiment_nrc, by = c("word_tokens" = "word")) %>%
filter(nrc != "positive" & nrc != "negative")
words_df <- tibble(text = news_nlp)
View(words_df)
words_df <- tibble(text = news_nlp) %>%
mutate_all(as.character)
text_df <- as.character(news_nlp$title)
text_df <- tibble(text = text_df)
View(text_df)
text_df <- text_df %>%
unnest_tokens(word_tokens, text, token="words") %>%
anti_join(stop_words, by = c("word_tokens" = "word")) %>%
left_join(sentiment_nrc, by = c("word_tokens" = "word")) %>%
filter(nrc != "positive" & nrc != "negative")
View(text_df)
png(file.path(path_plot,"NRC_plot.png"))
ggplot(data = filter(text_df, !is.na(nrc))) +
geom_histogram(aes(nrc), stat = "count", fill="Navy") +
scale_x_discrete(guide = guide_axis(angle = 45)) +
labs(title = "Overall sentiment analysis: Mexican news about out-of-pocket",
subtitle = "NRC method") +
theme_minimal()
dev.off()
